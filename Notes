Static pods: the pods which are running on a specific node and is managed but kubelet
-----------

Probes:
=======

Liveness probe: which indicates if the container is running
rediness probe indicates if the container is ready to serve request 
startup probe indicates if the application within the cluster is started

Mechanism to check :

httpget
tcpsocket
exec
grpc

Services:
=========
services are most commonly used to abstract acess to pods but it can also support other backend types running outside cluster using endpoints and without selector.

When Kubernetes processes a service description, and if the service selector matches a pod label, Kubernetes will automatically create an `Endpoints` object with the same name as the service, which stores the podâ€™s IP address and port

`Endpoints` are used to connect a service with backend pods or services

To define a headless service(without selector) add `clusterIP: None` in spec of the service whereas for service with selector add `type` of the service

Different types of services with selectors:
`NodePort`
`ClusterIP`
`LoadBalancer`

CNI:
===
It is an interface between container runtime interface and network. It also configures network routes.

All containers within a pod have same ip address which is provided by CNI but the container are configured with different ports.

all the containers within a pod can communicate with each other using localhost.

All pods and nodes can communication with other pods without NAT as the ip of the pods is same in a cluster which is managed by CNI.

NetworkPolicy
-------------
It is used to manage network policies between pods

HPA:
----

HPA is Horizontal pod autoscaler which increases the no. of replicas based on CPU and memory utilization.

Metrics server is required for HPA which captures metrics 

It takes sometime for metrics to be reflected in hpa.

By default 5 mins is the cooldown period.

cpu and memory limits has to be set in order to configure hpa and scale with memory.

Issue:-
------
Metrics server unable to capture metrics properly, if it HA is not setup 

Hpa was unable to fetch memory when app was deployed with argocd due to API version issue

Resolution:
-----------
change the apiversion from autoscaling/v2 to autoscaling v2beta1 when app is deployed using argocd.

VPA:
====

VPA is vertical pod autoscalar which increases/decreases the resources of a pod by destroying old pod and creating a new pod.

VPA is not installed by default in the cluster. Its a custom resource definition. Hence, it has to be installed separately.

3 components which get deployed as a part it are :-

1. Recommender             # recommends the change in pods resource request required
2. Admission Controller    # connect with apiserver to get the config for vpa
3. Updater                 # update the resource recommended by recommender

Sample yaml:
-----------
apiVersion: autoscaling.k8s.io/v1         # api version which needs to be present
kind: VerticalPodAutoscaler               # will be deployed as apart of crd
metadata:
  name: my-rec-vpa
spec:
  targetRef:                              # defining the target resource
    apiVersion: "apps/v1"                 # api version of the object
    kind:       Deployment                # target object type
    name:       my-rec-deployment         # target object name
  updatePolicy:
    updateMode: "Off"                     # off is when it recommendation but doesnt update
                                          # auto updates the pods as well

Prometheus is having three components:-
--------------------------------------
1. Http Server
2. Retrieval
3. storage


Http server is used for getting metrics from storage by prometheus
Retrieval is used to fetch metrics from pods. It uses a pull mechanism
only with short lived process push mechanism is used


Install Prometheus using helm
==============================

1. Add a repo 
   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

2. Update repo 
   helm repo update

3. Install kube Prometheus stack which includes alert manager, grafana, prometheus operator, state metrics, prometheus, node exporter.
    
    helm install <name> prometheus-community/kube-prometheus-stack  -n <namespace>

Service Monitor : Service monitor describes the set of targets to be monitored by Prometheus.
---------------

List of the service monitor
---------------------------

kubectl get servicemonitor


label release: prometheus is very important in the service monitor as prometheus find the service monitor and registers to the server and starts scarping


Exporter is a translator which converts application data to prometheus understandable metrics and expose /metrics 
-------- for prometheus to scrape

Node Exporter translater the metrics of the cluster nodes
-------------


Adding target
-------------



CGroups are responsible metering and limiting for allocating resources.
-------

Memory Cgroup
=============
It has  a soft limit and hard limit.

If the hard limit is reached, the process get killed randomly in that cgroup level which is the reason for putting only one service in a container which improves granularity and if the limit is reached, it process in that specific container is killed.

Soft limits are not enforced. Containers works fine above the soft limit. However, if the host is going OOM and need more memory then, it looks at the cgroups which are having usage above soft limit and the memory is taken from there by the kernel.


CPU CGroup:
===========
keeps a track of cpu usage
no limits but can be set as percent
Dedicate CPU to process so that the process doesnt  move from one cpu to another

Block IO
========
keeps track of I/O for each group
set throttle(limits) for each group

Net_cls and net_prio  cgroup
============================

Devices cgroup
==============
controls which group can read and write on device nodes

Freezer cgroup
==============


Namespace
==========
it limits what you can see 

multiple namespaces
-------------------
pid
mnt
net
uts
ipc
user

each process is in one namespace of each type ( a process will be in pid namespace and then mnt namespace and so on).

pid namespace: process within PID namespace can see only process in the same PID namespace
-------------
net namespace: process within a network namespace get their own private network stack
-------------

mnt namespace lets each container mount anything which will be visible in that container.
--------------

uts namespace lets a unique hostname
-------------

ipc namespace allows a process to have their own IPC semaphores, IPC shared memory and IPC message queues.

User namespace: maps UID within a container.  
--------------
root in the container is different from root in host.


new ns is created when new process is created and extra flag is added to process that it has to be in new ns.
it is present in /proc/pid/ns

Copy on write
--------------


Container runtimes using CGroups and namespaces
------------------------------------------------

systemd recently added support for docker


performance will be same on all cri
