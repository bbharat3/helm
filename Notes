Static pods: the pods which are running on a specific node and is managed but kubelet
-----------

Probes:
=======

Liveness probe: which indicates if the container is running
rediness probe indicates if the container is ready to serve request 
startup probe indicates if the application within the cluster is started

Mechanism to check :

httpget
tcpsocket
exec
grpc

Services:
=========
services are most commonly used to abstract acess to pods but it can also support other backend types running outside cluster using endpoints and without selector.

When Kubernetes processes a service description, and if the service selector matches a pod label, Kubernetes will automatically create an `Endpoints` object with the same name as the service, which stores the podâ€™s IP address and port

`Endpoints` are used to connect a service with backend pods or services

To define a headless service(without selector) add `clusterIP: None` in spec of the service whereas for service with selector add `type` of the service

Different types of services with selectors:
`NodePort`
`ClusterIP`
`LoadBalancer`

`StatefulSet`:

CNI:
===
It is an interface between container runtime interface and network. It also configures network routes.

All containers within a pod have same ip address which is provided by CNI but the container are configured with different ports.

all the containers within a pod can communicate with each other using localhost.

All pods and nodes can communication with other pods without NAT as the ip of the pods is same in a cluster which is managed by CNI.

NetworkPolicy
-------------
It is used to manage network policies between pods

HPA:
----

HPA is Horizontal pod autoscaler which increases the no. of replicas based on CPU and memory utilization.

Metrics server is required for HPA which captures metrics 

It takes sometime for metrics to be reflected in hpa.

By default 5 mins is the cooldown period.

cpu and memory limits has to be set in order to configure hpa and scale with memory.

Issue:-
------
Metrics server unable to capture metrics properly, if it HA is not setup 

Hpa was unable to fetch memory when app was deployed with argocd due to API version issue

Resolution:
-----------
change the apiversion from autoscaling/v2 to autoscaling v2beta1 when app is deployed using argocd.

VPA:
====

VPA is vertical pod autoscalar which increases/decreases the resources of a pod by destroying old pod and creating a new pod.

VPA is not installed by default in the cluster. Its a custom resource definition. Hence, it has to be installed separately.

3 components which get deployed as a part it are :-

1. Recommender             # recommends the change in pods resource request required
2. Admission Controller    # connect with apiserver to get the config for vpa
3. Updater                 # update the resource recommended by recommender

Sample yaml:
-----------
apiVersion: autoscaling.k8s.io/v1         # api version which needs to be present
kind: VerticalPodAutoscaler               # will be deployed as apart of crd
metadata:
  name: my-rec-vpa
spec:
  targetRef:                              # defining the target resource
    apiVersion: "apps/v1"                 # api version of the object
    kind:       Deployment                # target object type
    name:       my-rec-deployment         # target object name
  updatePolicy:
    updateMode: "Off"                     # off is when it recommendation but doesnt update
                                          # auto updates the pods as well

`Prometheus` is having three components:-
--------------------------------------
1. Http Server
2. Retrieval
3. storage


Http server is used for getting metrics from storage by prometheus.

Retrieval is used to fetch metrics from pods. It uses a pull mechanism.

only with short lived process push mechanism is used


Install Prometheus using helm
==============================

1. Add a repo 
   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

2. Update repo 
   helm repo update

3. Install kube Prometheus stack which includes alert manager, grafana, prometheus operator, state metrics, prometheus, node exporter.
    
    helm install <name> prometheus-community/kube-prometheus-stack  -n <namespace>

`Service Monitor` : Service monitor describes the set of targets to be monitored by Prometheus.

if `serviceMonitorSelectorNilUsesHelmValues: true` is set in values.yaml for the helm chart
`serviceMonitorSelector:<somelabel>` is used to scrape metrics which is configured in the prometheus.yaml in helm chart. This setting is a part of crd prometheuses.monitoring.coreos.com.

`serviceMonitorSelector` means search all the service monitors which has the matching label else it will not be discovered by prometheus

Sample Code from prometheus.yaml
================================
<!-- {{ else if .Values.prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues  }}
  serviceMonitorSelector:
    matchLabels:
      release: {{ $.Release.Name | quote }} -->

In the above code release label is set to release name of the helm chart deployed which is provided by user in the below command

>> helm install <re>


List of the `service monitor`
---------------------------

kubectl get servicemonitor


label `release: prometheus` is very important in the service monitor as prometheus find the service monitor and registers to the server and starts scarping.



`Exporter` is a translator which converts application data to prometheus understandable metrics and expose /metrics for prometheus to scrape

Node Exporter translater the metrics of the cluster nodes
-------------


`Adding target`

Prometheus needs 3 things to scrape metrics
============================================
1.  `Exporter application`: It exposes /metrics enpoint
2.  `Exporter service`: service which will connect to the exporter 
3.  `Servicemonitor`: to be discovered by prometheus 



`CGroups` are mechanism responsible metering and limiting for allocating resources.
These are a way to classify all the process running in a system in a hierarchical groups

Memory Cgroup
=============
It has  a `soft limit and hard limit`.

If the hard limit is reached, the process get killed randomly in that cgroup level which is the reason for putting only one service in a container which improves granularity and if the limit is reached, it process in that specific container is killed.

Soft limits are not enforced. Containers works fine above the soft limit. However, if the host is going OOM and need more memory then, it looks at the cgroups which are having usage above soft limit and the memory is taken from there by the kernel.


CPU CGroup:
===========
keeps a track of cpu usage
no limits but can be set as percent
Dedicate CPU to process so that the process doesnt  move from one cpu to another

Block IO
========
keeps track of I/O for each group
control and monitor access to block devices.
set throttle(limits) for each group

Net_cls and net_prio  cgroup
============================

Devices cgroup
==============
controls which group can read and write on device nodes

Freezer cgroup
==============
This is used while migrating a container from one system to another system then the cgroup is freezed which means all the processes are freezed before migration and once the migration is complete unfreeze command is sent to the entire cgroup



Namespace
==========
it limits what you can see. It is a kernel feature to group task into namespace of a kind.
these are the particular view of a process

`unshare` command is used to create a namespace in linux 

multiple namespaces
-------------------
pid
mnt
net
uts
ipc
user
cgroups

each process is in one namespace of each type ( a process will be in pid namespace and then mnt namespace and so on).

`pid namespace`: process within PID namespace can see only process in the same PID namespace
Process in a namespace start with PID 1 which has a different PID on the host machine

`net namespace`: process within a network namespace get their own private network stack. A particular device can be connected to only 1  namespace

`Internet access to container:`

create a veth outside the main system and attach it to  the container. Provide and IP address to the veth and the main system and create a bridge connection between them.


`mnt namespace` lets each container mount anything which will be visible in that container.
--------------

`uts namespace` lets a unique hostname
-------------

`ipc namespace` allows a process to have their own IPC semaphores, IPC shared memory and IPC message queues.
--------------

`User namespace`: maps UID within a container.  
--------------
root in the container is different from root in host.

`persistent namespace`


new ns is created when new process is created and extra flag is added to process that it has to be in new ns.
it is present in /proc/pid/ns

Copy on write
--------------


Container runtimes using CGroups and namespaces
------------------------------------------------

systemd recently added support for docker


performance will be same on all cri
